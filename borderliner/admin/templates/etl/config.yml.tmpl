type: 'ETL'

cloud: cloud.yml

pipeline_name: {pipeline_name}
# set true i'll force pipeline to pass data through csv dumps
# and don't hold the entire dataset on memory.
# some database supports COPY method from cloud storage
# you can define the insertion_method: copy on target
dump_data_csv: True

generate_control_columns: True

create_target_tables: False

alchemy_log_level: CRITICAL

source:
  source_type: DATABASE
  type: {source_type}
  host: {source_host}
  {% if source_port %}
  port: {source_port}
  {% endif %} 
  database: {source_database}
  {% if source_schema %}
  schema: {source_schema}
  {% endif %}
  username: {source_username}
  password: {source_password}
  connection_try: 4
  sleep_try: 2
  table: {source_table}
  connection: new
  {% if chunk_size %}
  chunk_size: {chunk_size}
  {% endif %}
  queries:
    {% if source_query %}
    extract: {source_query}
    {% else %}
     extract: SELECT * from public.{source_table} order by {iteration_field},period limit 10000; 
    {% endif %}
    {% if iteration_field %}
    iterate: select distinct {iteration_field} from public.{source_table} limit 10;
    {% endif %}

target:
  target_type: DATABASE
  type: {target_type}
  host: {target_host}
  {% if target_port %}
  port: {target_port}
  {% endif %}
  database: {target_database}
  {% if target_schema %}
  schema: {target_schema}
  {% endif %}
  username: {target_username}
  password: {target_password}
  connection_try: 4
  sleep_try: 2
  table: {target_table}
  connection: new
  {% if chunk_size %}
    chunk_size: {chunk_size}
  {% endif %}
  {% if insertion_method %}
    insertion_method: {insertion_method}
  {% endif %}
  {% if conflict_key %}
    conflict_key: {conflict_key}
  {% endif %}
  {% if conflict_action %}
    conflict_action: {conflict_action}
  {% endif %}